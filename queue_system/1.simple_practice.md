# Practice

Time to practice what we've learned. We will create a simple queue to handle data pre-processing.

## The goal

We will create a simple API that will send data to a queue. We will create a worker that will process the data and store them in a database (a JSON file in our case).

## Where to start?

Before you dive into this exercise, go over [this article](https://towardsdatascience.com/kafka-docker-python-408baf0e1088) *(you can try [this](https://archive.ph/M8hQT) link if stoped by a paywall)*. It will give you the basics to approach this exercise.

## Context
You are working for Delhaize, each time a customer buys something, your API will receive this kind of data:

```JSON
{
    "id": "123456789",
    "store": "Brussels",
    "date": "2020-01-01",
    "products": [
        {
            "id": "123456789",
            "name": "Banana",
            "price": 1.5
        },
        {
            "id": "123456789",
            "name": "Bread",
            "price": 1.5
        },
        {
            "id": "123456789",
            "name": "Water",
            "price": 1.5
        }
    ]
}
```

Your manager would like to perform some analytics on the data. She wants to know which proportion of the products are `Fruits`, `Bakery` or `Drinks`. She also wants to know the average price of the products. She also wants to know the average price of the products per store.

For the prices, just need to compute the total price of the basket, the data is already in the right format. But for the categories, we need to pre-process the data. We need to add a category field to each product.

Your manager is a nice person, she will give you the categories of each product. You can find it below.

*Note that the products in the list are not capitalized, you will need to handle that when processing the data.*

```JSON
{
    "Fruit": "an apple, banana, orange, pear, kiwi",
    "Bakery": "bread, croissant, baguette, cake",
    "Drink": "water, soda, beer, wine"
}
```

Good for you, it's a small shop with a small number of products.

After processing, your data should look something like that:

```JSON
{
    "id": "123456789",
    "store": "Brussels",
    "date": "2020-01-01",
    "total_price": 4.5,
    "products": [
        {
            "id": "123456789",
            "name": "Banana",
            "price": 1.5,
            "category": "Fruit"
        },
        {
            "id": "123456789",
            "name": "Bread",
            "price": 1.5,
            "category": "Bakery"
        },
        {
            "id": "123456789",
            "name": "Water",
            "price": 1.5,
            "category": "Drink"
        }
    ]
}
```

## Github repository

Create a new GitHub repository and clone it on your computer.

## Run Kafka with Docker

We will use Docker to run Kafka. You can find the documentation [here](https://kafka.apache.org/quickstart).

Let's not focus on all the environment variables, we will just use the default ones.

Create a `docker-compose.yml` file with the following content:

```yaml
version: '2'
services:
  zookeeper:
    image: wurstmeister/zookeeper:3.4.6
    ports:
     - "2181:2181"
  kafka:
    image: wurstmeister/kafka
    ports:
     - "9092:9092"
    expose:
     - "9093"
    environment:
      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9093,OUTSIDE://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_LISTENERS: INSIDE://0.0.0.0:9093,OUTSIDE://0.0.0.0:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CREATE_TOPICS: "topic_test:1:1"
    volumes:
     - /var/run/docker.sock:/var/run/docker.sock
```

Run the following command to start Kafka:

```bash
docker-compose up -d
```

You can check that Kafka is running by running the following command:

```bash
docker-compose logs -f kafka
```

You can create a topic called `delhaize_shop` with:
    
```bash
docker-compose exec kafka kafka-topics.sh --create --topic delhaize_shop --bootstrap-server localhost:9092
```

You can check that the topic has been created with:

```bash
docker-compose exec kafka kafka-topics.sh --list --bootstrap-server localhost:9092
```

You're good to go! Easy, right?


## The API

We will create a simple API that will receive the data and send them to a queue. We will use [FastAPI](https://fastapi.tiangolo.com/) to create the API.


### Create the API

Create a new folder `api` and create a new file `api.py` in it.

Install FastAPI, uvicorn and kafka-python:

```bash
pip install fastapi==0.85.0 uvicorn==0.18.3 kafka-python==2.0.2
```

Add the following code to `api.py`:

```python
from fastapi import FastAPI, Request

app = FastAPI()

@app.post("/data")
async def data(user_data: dict):
   
    print(user_data)
    # TODO: send data to the queue
    # Add your code here


    return {"status": "ok"}
```

## The consumer

* Create a `database.json` that will store the data.
* Create a `consumer.py` file that will process the messages and store them in the JSON file. 

## Testing time!

Now that we have our API and our consumer, we can test them.

Make sure you have Kafka, your API, and your consumer running.

First, make sure your API is working using postman or curl.

Check that the data is correctly stored in the `database.json` file.

Now, let's perform a stress test.

Create a `stress_test.py` file that will send 1000 requests to the API.
Here is the code:

```python
from requests import Session

with Session as sess:
    for i in range(1000):
        data = {
            "id": i,
            "store": "Brussels",
            "date": "2020-01-01",
            "products": [
                {
                    "id": "123456789",
                    "name": "Banana",
                    "price": 1.5
                },
                {
                    "id": "123456789",
                    "name": "Bread",
                    "price": 1.5
                },
                {
                    "id": "123456789",
                    "name": "Water",
                    "price": 1.5
                }
            ]
        }
        sess.post("http://localhost:8000/data", json=data)
```

Run the stress test and check that the data is correctly stored in the `database.json` file. For that, you can run this piece of code to see how many items are stored in your JSON file:
    
```python
import JSON

with open("database.json", "r") as f:
    data = json.load(f)

print("Items in the DB: ", len(data))
```

Also, make sure that the items contain the categories and the total price.

## Bonus

* Use a real database instead of a JSON file.
* Use a cloud provider to run Kafka and your API. *([start here](https://developer.confluent.io/learn-kafka/apache-kafka/get-started-hands-on/))*

## Resources

* [Kafka documentation](https://kafka.apache.org/quickstart)
* [FastAPI documentation](https://fastapi.tiangolo.com/)
* [Kafka with Python](https://towardsdatascience.com/kafka-python-explained-in-10-lines-of-code-800e3e07dad1)
